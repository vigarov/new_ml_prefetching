{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.prepro import single_preprocess,ExtraProcessCodeInfo,check_correct_dir_str,check_correct_dir,RunIdentifier\n",
    "from utils.fltrace_classes import *\n",
    "from utils.constants import *\n",
    "from collections import defaultdict\n",
    "from models.NLinear import NLinear\n",
    "from functools import cached_property\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy.typing as npt\n",
    "from typing import Sequence\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "import pickle as pkl\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "import numpy as np\n",
    "from plotly import graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2024\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = \"/home/garvalov/mlp2/data/data/prediction_select_few/processed/\"\n",
    "TEST_DF = IN_PATH+\"facesim/processed_500_125.csv\"\n",
    "OUT_PATH = \"/home/garvalov/mlp2/prediction/train_outputs/\"\n",
    "MODEL_BASE_DIR = OUT_PATH+\"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self,config_or_dict,name:str=\"\",name_features:list|None=None,update_dict:dict|None=None):\n",
    "        self.name = name\n",
    "        if name_features is None:\n",
    "            name_features = ['epochs']\n",
    "        elif 'epochs' not in name_features:\n",
    "            name_features = ['epochs'] + name_features\n",
    "        self.name_features = name_features\n",
    "        if config_or_dict is None:\n",
    "            config_or_dict = Config.get_default_train_config()\n",
    "        if isinstance(config_or_dict,Config):\n",
    "            config_dict = config_or_dict.config_dict\n",
    "        else:\n",
    "            config_dict = config_or_dict\n",
    "        if update_dict is not None:\n",
    "            config_dict.update(update_dict)\n",
    "        assert 'epochs' in config_dict\n",
    "        self.config_dict = config_dict\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.config_dict[key]\n",
    "    \n",
    "    def __contains__(self,key):\n",
    "        return key in self.config_dict\n",
    "    \n",
    "    def __getattr__(self,k):\n",
    "        if k not in self:\n",
    "            raise AttributeError()\n",
    "        return self[k] # This would've otherwise raised a KeyError, which is not the expected Python behaviour when fetching a non-present attribute\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_default_train_config():\n",
    "        return Config({\n",
    "            'tt_split': 0.75, # Train-test split\n",
    "            'bs': 8, # Training batch size\n",
    "            'base_lr': 1*(10**-1), # Starting learning rate,\n",
    "            'end_lr': 1*(10**-3), # Smallest lr you will converge to\n",
    "            'epochs': 10, # epochs\n",
    "            'warmup_epochs': 3 # number of warmup epochs\n",
    "            }) \n",
    "    @cached_property\n",
    "    def ident(self) -> str:\n",
    "        return ('_'.join([self.name]+[str(self.config_dict[feature]) for feature in self.name_features])).lower()\n",
    "    \n",
    "    def update(self,other,name_merge=\"keep_base\"):\n",
    "        assert isinstance(other,Config) or isinstance(other,dict)\n",
    "        assert name_merge in [\"keep_base\",\"keep_new\",\"concat\"]\n",
    "        return Config(self.config_dict.update(other.config_dict if isinstance(other,Config) else other),self.name if name_merge == \"keep_merge\" or isinstance(other,dict) else (other.name if name_merge == \"keep_new\" else other.name+'_'+self.name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlinear_config(deltas=False,new_only=True,shuffle=False):\n",
    "    return Config(None,update_dict={\n",
    "        \"seq_len\":10,\n",
    "        \"pred_len\":10,\n",
    "        \"individual\": False,\n",
    "        \"deltas\": deltas,\n",
    "        \"shuffle\":shuffle,\n",
    "        \"new_only\": new_only\n",
    "    },name=\"nlinear\",name_features=[\"seq_len\",\"pred_len\",\"deltas\",\"new_only\",\"shuffle\"])\n",
    "\n",
    "def get_nlinear_model(config):\n",
    "    return NLinear(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric():\n",
    "    def __init__(self,name:str,fn,better_direction : str):\n",
    "        self.name:str = name\n",
    "        self.fn=fn\n",
    "        assert better_direction in [\"lower\",\"higher\"]\n",
    "        self.better_direction = better_direction\n",
    "\n",
    "    def __call__(self, *args, **kwds):\n",
    "        return self.fn(*args,**kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(gt,preds):\n",
    "    return np.sqrt(np.mean((preds-gt)**2))\n",
    "\n",
    "def mae(gt,preds):\n",
    "    return np.mean(np.abs(gt - preds))\n",
    "\n",
    "def _validate_or_get_K(preds: npt.NDArray,K):\n",
    "    if K is not None:\n",
    "        assert K == len(preds)\n",
    "    else:\n",
    "        K = len(preds)\n",
    "    return K\n",
    "def precision_at_K(gt: npt.NDArray, preds: npt.NDArray, K = None):\n",
    "    K = _validate_or_get_K(preds,K)\n",
    "    return len(np.intersect1d(preds,gt))/K\n",
    "\n",
    "def success_at_K(gt: npt.NDArray,preds: npt.NDArray):\n",
    "    assert isinstance(gt,np.ndarray), f\"got `trues` of type {type(gt)}\"\n",
    "    true = gt[:,0]\n",
    "    assert isinstance(true,np.ndarray)\n",
    "    return np.isin(true,preds).sum()/len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALL_METRICS = [\n",
    "    Metric(\"Success\",success_at_K,\"higher\"),\n",
    "    Metric(\"P@10\",precision_at_K,\"higher\"),\n",
    "    Metric(\"RMSE\",rmse,\"lower\"),\n",
    "    Metric(\"MAE\",mae,\"lower\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_batches(sequence:Sequence|pd.Series,history_window_size:int,output_window_size:int = -1,hist_remove_after=False,outputs_fill_after=True,translate_to_page_num = True) -> tuple[npt.NDArray,npt.NDArray|None]:\n",
    "    # Given a sequence of addresses, translates them to pages, and builds a sequence of histories and optionally desired outputs.\n",
    "    # The last element of the built history can be considered as the faulted page\n",
    "    # That is, consider the input sequence as [A_0,A_1,...,A_i,A_i+1,...,A_n],\n",
    "    # The output is \n",
    "    #[\n",
    "    #   [A_0,A_1,...,A_(history_window_size-1)],  ---> corresponds to the `history_window_size` faulty addresses\n",
    "    #   [A_1,...,A_(history_window_size)],\n",
    "    #   [A_2,...,A_(history_window_size+1)],\n",
    "    #   ...,\n",
    "    #   \n",
    "    #]  \n",
    "    # Optionally,if output_window_size != -1, outputs a sequence of `output_window_size` next faulty addresses\n",
    "    # That is, outputs\n",
    "    # [\n",
    "    #   [A_(history_window_size),A_(history_window_size+1),...,A_(history_window_size+output_window_size-1)],   ---> corresponds to the next `output_window_size` faulty addresses\n",
    "    #   [A_(history_window_size+1),...,A_(history_window_size+output_window_size)],\n",
    "    #   ...,\n",
    "    #   [A_N-1,A_N,<FILLER>,...,<FILLER>].\n",
    "    #   [A_N,<FILLER>,...,<FILLER>],\n",
    "    #   [<FILLER>,<FILLER>,..,<FILLER>]\n",
    "    # ] of same length as the history output, where <FILLER> is simply the last element, repeated.\n",
    "    assert history_window_size > 0 and len(sequence) > history_window_size, f\"Sequence of length {len(sequence)}, yet {history_window_size} requested as history\"\n",
    "\n",
    "    is_pd = isinstance(sequence,pd.Series)\n",
    "    series = pd.Series(data=sequence) if not is_pd else sequence\n",
    "\n",
    "    # Translate to page numbers\n",
    "    if translate_to_page_num:\n",
    "        series = series.swifter.progress_bar(desc=\"Getting page number\").apply(get_page_num)\n",
    "    data=series.values\n",
    "    assert isinstance(data,np.ndarray)\n",
    "\n",
    "    n = len(sequence)\n",
    "    \n",
    "    idx = np.arange(history_window_size)[None, :] + np.arange(n - history_window_size - (output_window_size if hist_remove_after else 0) + 1)[:, None]\n",
    "    history_windows = data[idx]\n",
    "    output_windows = None\n",
    "    if output_window_size != -1:\n",
    "        out_idx = np.arange(output_window_size)[None, :] + np.arange(history_window_size, n-(output_window_size if not outputs_fill_after else 0)+1)[:, None]\n",
    "        output_windows = np.zeros(out_idx.shape)\n",
    "        valid_indices = out_idx < n\n",
    "        output_windows[np.where(valid_indices)] = data[out_idx[valid_indices]]\n",
    "        output_windows[~valid_indices] = data[-1]\n",
    "    \n",
    "    assert (\n",
    "            (hist_remove_after != outputs_fill_after          and len(history_windows) == len(output_windows)) or \n",
    "            (not hist_remove_after and not outputs_fill_after and len(history_windows) > len(output_windows))  or\n",
    "            (hist_remove_after and outputs_fill_after         and len(history_windows) < len(output_windows))\n",
    "        )\n",
    "\n",
    "    return pd.Series(data=list(history_windows)) if is_pd else np.array(history_windows), pd.Series(data=list(output_windows)) if is_pd else np.array(output_windows)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PageFaultDataset(Dataset):\n",
    "    def __init__(self,config, pd_serie,indices_split):\n",
    "        super().__init__()\n",
    "        self.x,self.y = build_batches(pd_serie.loc[indices_split],history_window_size=config.seq_len,output_window_size=config.pred_len,outputs_fill_after=False,hist_remove_after=True,translate_to_page_num=True)\n",
    "        self.x = self.x.values\n",
    "        self.y = self.y.values\n",
    "        assert len(self.x) == len(self.y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        :return: the number of elements in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index) -> dict:\n",
    "        return torch.tensor(self.x[index],dtype=torch.double).unsqueeze(-1),torch.tensor(self.y[index],dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tt_ds(config):\n",
    "    df_to_use = pd.read_csv(TEST_DF)\n",
    "    if config.new_only:\n",
    "        df_to_use = df_to_use[df_to_use[\"flags\"] < 32]\n",
    "    df_to_use = df_to_use[\"addr\"]\n",
    "    if config.deltas:\n",
    "        df_to_use = df_to_use.diff().dropna()\n",
    "    df_to_use = df_to_use.astype(int)\n",
    "    train_tensor_size = int(config[\"tt_split\"] * len(df_to_use))\n",
    "    train_ds = PageFaultDataset(config,df_to_use,df_to_use.index[:train_tensor_size])\n",
    "    test_ds = PageFaultDataset(config,df_to_use,df_to_use.index[train_tensor_size:])\n",
    "    return DataLoader(train_ds,batch_size=config.bs,shuffle=config.shuffle), DataLoader(test_ds,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model,eval_dataloader,device,metrics: list|None=None, print_validation=True):\n",
    "    model.eval()\n",
    "    if metrics is None:\n",
    "        metrics = ALL_METRICS\n",
    "    gt = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in (tqdm(eval_dataloader,desc=\"Evaluating model\",total=len(eval_dataloader)) if print_validation else eval_dataloader):\n",
    "            x,y = batch\n",
    "            predicted = model(x).squeeze(dim=-1).detach().cpu()\n",
    "            if isinstance(y,list):\n",
    "                assert isinstance(predicted,list)\n",
    "                gt.extend(y)\n",
    "                preds.extend(predicted.numpy().tolist())\n",
    "            else:\n",
    "                gt.append(y)\n",
    "                preds.append(predicted)\n",
    "    gt = np.array(gt).squeeze()\n",
    "    preds = np.array(preds).squeeze()\n",
    "    print(\"Computed batches for all of eval dataset\")\n",
    "    all_res = [metric(gt,preds) for metric in (tqdm(metrics,desc=\"Computing metric results\",total=len(metrics)) if print_validation else metrics)]\n",
    "    return all_res\n",
    "\n",
    "def maybe_update_result(current_value,new_result,better=\"lower\"):\n",
    "    assert better in [\"lower\",\"higher\"]\n",
    "    updated = False\n",
    "    new_value = current_value\n",
    "    if current_value == -np.inf or (better == \"lower\" and new_result <= current_value) or (better == \"higher\" and new_result >= current_value):\n",
    "        updated = True\n",
    "        new_value = new_result\n",
    "    return updated,new_value\n",
    "                \n",
    "\n",
    "def generic_train_loop(config:Config,model_fn,override_previous_dir=False,print_validation = True,tqdm_train=True):\n",
    "    # Trains a model\n",
    "    # When `override_previous_dir` = True, destroy the directory of the previous saved run with the same config id (if it exists)\n",
    "    device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "    device = torch.device(device)\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(SEED)\n",
    "    train_dataloader, test_dataloader = get_tt_ds(config)\n",
    "    epochs = config[\"epochs\"]\n",
    "    warmup_epochs = config['warmup_epochs']\n",
    "    model = model_fn(config).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['base_lr'], eps=1e-6,amsgrad=True)\n",
    "    loss_fn = torch.nn.MSELoss()    \n",
    "\n",
    "    # LR scheduler\n",
    "    # We do the following technique : high LR in the beginning, low towards the end\n",
    "    # starting from base_lr we decrease up to e-5, by a factor of 1/sqrt(10) ~0.3162  k times\n",
    "    fct = 1/np.sqrt(10)\n",
    "    final_lr = config[\"end_lr\"]\n",
    "    end_epoch = min(32,epochs)\n",
    "    num_groups = math.ceil(math.log(final_lr / config[\"base_lr\"], fct))\n",
    "    group_size = end_epoch // num_groups\n",
    "    milestones = [warmup_epochs+group_size*i for i in range(num_groups)]\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones,fct)\n",
    "\n",
    "\n",
    "    metrics_to_use = ALL_METRICS\n",
    "    best_results = {metric:-np.inf for metric in metrics_to_use}\n",
    "    model_bases = Path(MODEL_BASE_DIR)\n",
    "    model_bases.mkdir(parents=True,exist_ok=True)\n",
    "    save_dir = model_bases / config.ident\n",
    "    if override_previous_dir and save_dir.exists():\n",
    "        assert save_dir.is_dir(),f\"{save_dir.absolute().as_posix()} exists and is not a directory!\"\n",
    "        rmtree(save_dir.absolute().as_posix())\n",
    "    save_dir.mkdir(parents = False,exist_ok=False)\n",
    "    for metric in metrics_to_use:\n",
    "        (save_dir/metric.name).mkdir(parents = False,exist_ok=False)\n",
    "\n",
    "    all_losses = []\n",
    "    all_results = defaultdict(list)\n",
    "    worse_success_count = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        train_iterator = tqdm(train_dataloader,desc=f\"Processing epoch {epoch:02d} w/ lr ({lr_scheduler.get_last_lr()})\",total=len(train_dataloader)) if tqdm_train else train_dataloader\n",
    "        c = 0\n",
    "        gl = 0\n",
    "\n",
    "        # Train\n",
    "        for batch in train_iterator:\n",
    "            x,y = batch\n",
    "            prediction = model(x).squeeze(dim=-1)\n",
    "            loss = loss_fn(prediction,y)\n",
    "            c+=1\n",
    "            cl = loss.item()\n",
    "            gl += cl\n",
    "            all_losses.append(cl)\n",
    "            if tqdm_train:\n",
    "                train_iterator.set_postfix({\"loss\": f\"{gl/c:6.3e}\"})\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Validate\n",
    "        if print_validation:\n",
    "            print(TEXT_SEPARATOR)\n",
    "        all_current_results = validate_model(model,test_dataloader,device,metrics_to_use,print_validation=print_validation)\n",
    "        for metric,new_metric_result in zip(metrics_to_use,all_current_results):    \n",
    "            if print_validation:\n",
    "                print(f\"{metric.name}: {new_metric_result}\\n\")\n",
    "            all_results[metric].append(new_metric_result)\n",
    "            # Save best models\n",
    "            curr_metric_res = best_results[metric]\n",
    "            updated,new_res = maybe_update_result(curr_metric_res,new_metric_result,metric.better_direction)\n",
    "            best_results[metric] = new_res\n",
    "            if updated:\n",
    "                if metric.name.lower() == \"success\": \n",
    "                    worse_success_count = 0\n",
    "                # Save the model\n",
    "                metric_dir:Path = save_dir/metric.name\n",
    "                fname:Path = metric_dir/\"model.pt\"\n",
    "                if fname.exists():\n",
    "                    assert fname.is_file()\n",
    "                    fname.unlink()\n",
    "                torch.save(model.state_dict(),fname.absolute().as_posix())\n",
    "                with open((metric_dir/\"config.pkl\").absolute().as_posix(),\"wb\") as f:\n",
    "                    pkl.dump(config,f,pkl.HIGHEST_PROTOCOL)\n",
    "            elif metric.name.lower() == \"success\": \n",
    "                worse_success_count += 1\n",
    "        if print_validation:\n",
    "            print(TEXT_SEPARATOR)\n",
    "        \n",
    "        # Update LR\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if worse_success_count >=3:\n",
    "            print(f\"Reached worse success on validation set three epochs in a row, early stopping the training to avoid overfitting!\")\n",
    "            break\n",
    "        \n",
    "    return model,save_dir, all_results, all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_nlinear_config()\n",
    "last_model,run_save_dir,all_results_base, all_losses_base = generic_train_loop(config,get_nlinear_model,override_previous_dir = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_nlinear_config()\n",
    "tr,tst = get_tt_ds(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(tr))\n",
    "x = x.squeeze()\n",
    "x[0,0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro = pd.read_csv(TEST_DF)\n",
    "train_tensor_size = int(config[\"tt_split\"] * len(df_prepro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_prepro[df_prepro[\"flags\"] < 32][\"addr\"].astype(int).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_prepro[df_prepro[\"flags\"] < 32].copy()\n",
    "filtered_df['addr_diff'] = filtered_df['addr'].diff()\n",
    "\n",
    "fig = px.scatter(\n",
    "    filtered_df,\n",
    "    x=filtered_df.index,\n",
    "    y='addr_diff',\n",
    "    title=\"Address Differences Over Time\"\n",
    ")\n",
    "\n",
    "# Add line connecting the points\n",
    "fig.add_scatter(\n",
    "    x=filtered_df.index,\n",
    "    y=filtered_df['addr_diff'],\n",
    "    mode='lines',\n",
    "    line=dict(color='rgba(0,0,0,0.3)'),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Index\",\n",
    "    yaxis_title=\"Address Difference\",\n",
    "    showlegend=False,\n",
    "    yaxis=dict(\n",
    "        tickformat='.2e'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_hyperparameters():\n",
    "    # Generate all possible combinations of boolean hyperparameters\n",
    "    param_combinations = list(itertools.product([False, True], repeat=3))\n",
    "    \n",
    "    # Store results for each combination\n",
    "    all_experiments = {}\n",
    "    \n",
    "    for deltas, new_only, shuffle in param_combinations:\n",
    "        # Generate experiment name\n",
    "        exp_name = f\"d{int(deltas)}n{int(new_only)}s{int(shuffle)}\"\n",
    "        print('='*80)\n",
    "        print(f\"Starting {exp_name}\")\n",
    "        print('='*80)\n",
    "\n",
    "        # Get configuration and train model\n",
    "        config = get_nlinear_config(deltas, new_only, shuffle)\n",
    "        last_model, run_save_dir, all_results_base, all_losses_base = generic_train_loop(\n",
    "            config,\n",
    "            get_nlinear_model,\n",
    "            override_previous_dir=False\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_experiments[exp_name] = {\n",
    "            'results': all_results_base,\n",
    "            'losses': all_losses_base,\n",
    "            'params': {'deltas': deltas, 'new_only': new_only, 'shuffle': shuffle}\n",
    "        }\n",
    "    \n",
    "    # Plot results\n",
    "    plot_all_results(all_experiments)\n",
    "    plot_all_losses(all_experiments)\n",
    "    \n",
    "    return all_experiments\n",
    "\n",
    "def plot_all_results(all_experiments):\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Color scale for different experiments\n",
    "    colors = px.colors.qualitative.Set3\n",
    "    \n",
    "    for i, (exp_name, exp_data) in enumerate(all_experiments.items()):\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        for metric, results in exp_data['results'].items():\n",
    "            y = np.array(results)\n",
    "            x = np.arange(len(results))\n",
    "            \n",
    "            # Add line trace\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                name=f\"{exp_name} - {metric.name}\",\n",
    "                line=dict(color=color),\n",
    "                mode='lines'\n",
    "            ))\n",
    "            \n",
    "            # Add minimum point marker\n",
    "            min_idx = np.argmin(y)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[x[min_idx]],\n",
    "                y=[y[min_idx]],\n",
    "                name=f\"{exp_name} - {metric.name} min\",\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    symbol='x',\n",
    "                    size=10,\n",
    "                    color='red'\n",
    "                ),\n",
    "                showlegend=False\n",
    "            ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Metric Results Across Experiments\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Metric Value\",\n",
    "        hovermode='x unified',\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "def plot_all_losses(all_experiments):\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Color scale for different experiments\n",
    "    colors = px.colors.qualitative.Set3\n",
    "    \n",
    "    for i, (exp_name, exp_data) in enumerate(all_experiments.items()):\n",
    "        color = colors[i % len(colors)]\n",
    "        y = np.array(exp_data['losses'])\n",
    "        x = np.arange(len(y))\n",
    "        \n",
    "        # Calculate number of epochs based on data length\n",
    "        num_epochs = len(exp_data['results'][list(exp_data['results'].keys())[0]])\n",
    "        iterations_per_epoch = len(y) // num_epochs\n",
    "        \n",
    "        # Add raw loss trace with high transparency\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            name=f\"{exp_name} - Raw\",\n",
    "            line=dict(color=color, width=1),\n",
    "            opacity=0.3,\n",
    "            showlegend=False\n",
    "        ))\n",
    "        \n",
    "        # Add smoothed loss trace\n",
    "        y_smoothed = gaussian_filter1d(y, sigma=max(len(y)//200, 15))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x,\n",
    "            y=y_smoothed,\n",
    "            name=f\"{exp_name}\",\n",
    "            line=dict(color=color, width=2)\n",
    "        ))\n",
    "        \n",
    "        # Create custom tick labels for epochs\n",
    "        if num_epochs <= 10:\n",
    "            tick_vals = [i * iterations_per_epoch for i in range(num_epochs)]\n",
    "            tick_text = list(range(num_epochs))\n",
    "        else:\n",
    "            tick_indices = np.linspace(0, num_epochs - 1, 10, dtype=int)\n",
    "            tick_vals = [int(i * iterations_per_epoch) for i in tick_indices]\n",
    "            tick_text = tick_indices\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Training Loss Across Experiments\",\n",
    "        xaxis=dict(\n",
    "            title=\"Epoch\",\n",
    "            ticktext=tick_text,\n",
    "            tickvals=tick_vals\n",
    "        ),\n",
    "        yaxis_title=\"Loss\",\n",
    "        hovermode='x unified',\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = cross_validate_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
